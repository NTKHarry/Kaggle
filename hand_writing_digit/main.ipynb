{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "5      0       0       0       0       0       0       0       0       0   \n",
       "6      7       0       0       0       0       0       0       0       0   \n",
       "7      3       0       0       0       0       0       0       0       0   \n",
       "8      5       0       0       0       0       0       0       0       0   \n",
       "9      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "5       0  ...         0         0         0         0         0         0   \n",
       "6       0  ...         0         0         0         0         0         0   \n",
       "7       0  ...         0         0         0         0         0         0   \n",
       "8       0  ...         0         0         0         0         0         0   \n",
       "9       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         0         0         0         0  \n",
       "6         0         0         0         0  \n",
       "7         0         0         0         0  \n",
       "8         0         0         0         0  \n",
       "9         0         0         0         0  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"D:\\tfolder\\codingFile\\AIlearning\\soft_max\\train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = df.iloc[:5, 1:].values  # Chọn 5 hàng đầu tiên và bỏ cột nhãn\n",
    "labels = df.iloc[:5, 0].values  # Lấy nhãn của 5 hình ảnh đầu tiên\n",
    "\n",
    "# Chuyển đổi dữ liệu thành dạng hình ảnh 28x28\n",
    "images = images.reshape(-1, 28, 28)\n",
    "\n",
    "# Vẽ các hình ảnh\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].imshow(images[i], cmap='gray')  # Vẽ ảnh với màu xám\n",
    "    axes[i].set_title(f\"Label: {labels[i]}\")  # Ghi nhãn của mỗi hình ảnh\n",
    "    axes[i].axis('off')  # Tắt trục tọa độ\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(df.iloc[:, 0].values, dtype=torch.long).to(device)\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = .2, random_state = 42)\n",
    "X_train = (X_train/255).to(device)\n",
    "X_test = (X_test/255).to(device)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33600, 784]) torch.Size([8400, 784])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'layer1.weight': tensor([[ 0.0029, -0.0208,  0.0315,  ...,  0.0039, -0.0050, -0.0214],\n",
      "        [-0.0100,  0.0245,  0.0182,  ...,  0.0235,  0.0221,  0.0084],\n",
      "        [-0.0276,  0.0115, -0.0214,  ..., -0.0018,  0.0083, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0336, -0.0311, -0.0254,  ..., -0.0189,  0.0213, -0.0264],\n",
      "        [-0.0050, -0.0281, -0.0120,  ...,  0.0316, -0.0075, -0.0108],\n",
      "        [-0.0223,  0.0187, -0.0151,  ..., -0.0034, -0.0270, -0.0274]]), 'layer1.bias': tensor([ 4.7751e-03,  9.3870e-03,  2.6798e-02, -9.1928e-03, -1.2428e-02,\n",
      "         1.4247e-02, -1.4720e-02, -3.4685e-02, -1.2038e-02, -2.3835e-03,\n",
      "         2.6994e-02, -1.1362e-02, -3.2536e-02, -2.8360e-02, -1.4660e-02,\n",
      "        -6.0950e-03,  1.9115e-02,  5.1171e-03,  2.9208e-03, -3.8466e-03,\n",
      "         2.1179e-02,  2.2247e-02, -1.8837e-02,  1.6312e-02, -1.9745e-02,\n",
      "         6.0725e-03, -7.8824e-03, -2.1798e-02, -2.4602e-03, -2.2217e-02,\n",
      "         3.6394e-03, -1.4891e-02, -2.9027e-02,  3.1759e-02,  1.6610e-02,\n",
      "        -3.4605e-03,  1.5690e-02,  3.4181e-02, -3.2850e-02, -1.7285e-02,\n",
      "        -1.7125e-02, -3.1183e-02,  6.2936e-03, -2.4003e-02, -1.0723e-02,\n",
      "        -1.9473e-02, -2.5404e-02,  2.1127e-02, -1.6431e-02,  1.9634e-02,\n",
      "         1.5667e-03, -2.9686e-02,  9.7575e-03,  3.0508e-02, -3.3540e-02,\n",
      "        -1.9277e-02, -3.4726e-02, -3.3377e-02, -3.3020e-02,  2.4042e-02,\n",
      "        -6.5289e-03, -2.6258e-02,  1.4092e-02, -1.5609e-02, -9.7711e-03,\n",
      "        -2.4311e-02,  1.3697e-03,  1.3834e-02,  2.7213e-02,  1.7728e-02,\n",
      "         3.1145e-03, -5.6443e-03,  1.6451e-02, -2.9168e-02, -1.2671e-02,\n",
      "         2.9608e-02,  1.7182e-02, -1.5463e-02, -3.7425e-03, -1.0826e-02,\n",
      "        -1.9726e-02, -9.8693e-03,  1.8544e-02, -3.3348e-02, -4.5416e-03,\n",
      "         2.9897e-02,  3.1003e-02,  2.0479e-02,  2.7213e-02, -2.4225e-02,\n",
      "        -2.3823e-02, -8.5283e-03, -2.0062e-02, -8.0707e-03, -2.7293e-02,\n",
      "         1.4491e-02,  1.9867e-02,  2.3091e-02,  2.7264e-02, -9.1166e-03,\n",
      "         7.0357e-03,  6.3410e-03, -1.0557e-02, -3.4555e-02,  1.0046e-02,\n",
      "         1.2686e-02,  1.5060e-02,  2.1983e-02, -7.9492e-03, -5.9047e-03,\n",
      "        -2.5925e-03, -3.1394e-02,  3.0412e-02,  3.6133e-03,  1.7256e-04,\n",
      "         2.4711e-02,  1.7528e-02, -1.5727e-02,  1.9582e-02, -1.3253e-02,\n",
      "         3.0541e-02, -8.5706e-03,  2.4711e-03,  1.5240e-04,  3.3700e-02,\n",
      "         9.6282e-03,  1.0090e-02,  6.8535e-04,  8.0882e-03,  9.2646e-03,\n",
      "         3.3872e-02, -1.6342e-02,  1.2433e-02,  3.4999e-02, -4.9710e-03,\n",
      "         3.3477e-02,  8.0246e-03,  1.7670e-02,  2.0914e-02, -1.5050e-02,\n",
      "         1.3703e-02, -1.4168e-02,  3.1428e-03, -3.2053e-02, -1.6489e-02,\n",
      "        -3.4290e-02, -2.8982e-02, -2.7113e-02,  3.5527e-03,  3.3027e-02,\n",
      "         3.1987e-02, -1.0790e-02, -9.2024e-03, -2.4163e-04, -3.3561e-02,\n",
      "        -2.0101e-02,  5.5169e-03, -2.5187e-02,  2.0326e-02, -2.3852e-02,\n",
      "         1.9697e-02, -3.2120e-02,  7.3970e-03,  1.9817e-02, -1.9170e-02,\n",
      "         2.0197e-02, -1.5967e-02,  2.9205e-02,  1.8145e-02, -1.9552e-03,\n",
      "         2.1920e-02, -4.6332e-03, -5.4869e-03, -1.8867e-02,  8.1736e-03,\n",
      "         9.8919e-03,  1.9221e-02, -3.3640e-02, -2.8321e-02,  7.9327e-03,\n",
      "         2.9001e-02,  2.3526e-03,  7.6796e-03, -3.2945e-02,  2.4819e-02,\n",
      "         2.1133e-02, -3.3380e-02,  1.7473e-02,  2.6839e-02,  1.5723e-02,\n",
      "        -3.5259e-02, -1.3796e-02, -2.3610e-02,  2.7329e-02, -1.7197e-02,\n",
      "         1.0734e-02, -4.9853e-03,  1.0004e-02, -3.0132e-02, -2.3231e-02,\n",
      "        -1.5024e-02,  6.4112e-03,  9.3736e-05,  2.6280e-02,  1.6060e-02,\n",
      "         3.0525e-02, -3.0172e-03,  8.4741e-03, -5.2810e-03,  5.4372e-03,\n",
      "        -3.5222e-02, -1.0934e-02, -7.0321e-03,  3.9040e-03, -2.5944e-02,\n",
      "         3.3106e-03,  1.8179e-02,  1.4667e-02,  1.9401e-02, -1.6779e-02,\n",
      "        -2.3578e-02, -1.2999e-02,  2.3839e-02, -2.6759e-02, -3.0085e-02,\n",
      "        -1.3625e-03, -1.6398e-02,  1.5627e-02, -1.7206e-02,  7.4011e-03,\n",
      "        -8.0920e-03, -2.8300e-02,  3.0769e-02,  2.6874e-02,  2.1579e-02,\n",
      "        -2.9197e-02,  1.7590e-02, -2.2045e-02, -2.0572e-02,  2.1529e-02,\n",
      "        -2.9741e-02,  8.4345e-03, -1.1626e-02,  4.9916e-03, -2.9440e-02,\n",
      "        -1.5125e-02,  1.6283e-02,  2.0695e-02,  1.1608e-03,  2.3669e-02,\n",
      "        -2.3381e-02,  1.1466e-02, -2.0526e-02,  6.8206e-03,  3.8554e-03,\n",
      "         1.3561e-04, -2.1620e-02,  1.1410e-02,  3.5035e-02,  3.3511e-02,\n",
      "        -2.7267e-02,  3.3850e-02,  3.5031e-02, -3.5217e-03,  3.5199e-02,\n",
      "        -1.0062e-02, -2.6209e-02, -2.3937e-02, -3.0058e-02, -2.6019e-02,\n",
      "         2.8039e-02,  1.9247e-02, -5.2818e-03,  9.5729e-03, -3.1165e-02,\n",
      "        -1.9366e-04, -1.0578e-02, -3.1356e-02, -2.7701e-03,  1.9195e-02,\n",
      "         5.1572e-03,  1.0328e-03,  2.1416e-02, -3.2814e-02, -2.6638e-02,\n",
      "         8.4886e-03,  3.4210e-02, -3.2232e-02, -2.0694e-02, -3.3740e-02,\n",
      "        -2.6765e-02, -1.5255e-03, -2.2851e-02,  3.5109e-02,  1.3973e-02,\n",
      "        -2.9104e-02,  1.7287e-02,  7.3956e-03, -1.0467e-02,  2.7154e-02,\n",
      "        -3.0443e-02,  2.5720e-02,  1.3565e-02, -2.8002e-02, -1.8286e-02,\n",
      "         2.9829e-02,  3.3513e-02,  2.6498e-02,  7.3795e-04, -6.9185e-03,\n",
      "         2.7058e-02, -3.3309e-02,  2.1841e-02, -1.4879e-02, -3.4451e-02,\n",
      "        -1.9766e-02,  2.9033e-02,  3.3908e-02,  9.9613e-03,  4.7379e-03,\n",
      "         2.2312e-03, -7.7316e-03, -1.6701e-02,  2.8070e-02, -7.2687e-03,\n",
      "        -5.8034e-04,  3.2668e-02,  6.9998e-03,  9.9559e-03,  2.0994e-02,\n",
      "        -8.2038e-03, -1.7291e-02, -6.1157e-03,  2.8485e-02, -3.2843e-02,\n",
      "         3.0997e-02, -6.4742e-03, -3.5471e-02, -2.0172e-02, -1.9702e-02,\n",
      "        -2.6877e-03, -3.2492e-02,  2.5726e-02,  2.9039e-02, -1.1310e-02,\n",
      "         3.0085e-03,  1.3172e-02, -3.4006e-02,  1.6910e-02, -1.0576e-02,\n",
      "         1.8978e-02,  1.0449e-02,  2.0347e-02, -3.0229e-02, -2.3068e-03,\n",
      "        -1.7776e-02,  1.0640e-02, -1.4637e-02, -2.8870e-02, -1.2398e-02,\n",
      "        -2.0083e-02, -1.0581e-02, -3.1553e-02, -1.1754e-02,  1.6227e-02,\n",
      "        -1.5253e-02,  2.3587e-02,  2.5829e-03,  2.4315e-03,  3.3534e-02,\n",
      "        -2.5337e-02,  1.6086e-02,  3.0662e-02,  2.6125e-02,  7.7005e-03,\n",
      "         9.8919e-04, -2.9523e-02,  9.5908e-03,  2.6525e-02,  2.6294e-02,\n",
      "        -2.6572e-02, -3.4722e-02,  2.2528e-02,  5.9785e-03, -3.4782e-02,\n",
      "         1.8426e-02,  9.9326e-03, -1.6572e-02, -6.0550e-03, -1.5932e-02,\n",
      "        -6.2894e-03,  3.1857e-02,  2.0765e-02,  8.7105e-05, -3.3949e-02,\n",
      "        -2.8955e-02,  2.7168e-02, -2.3235e-02,  2.6263e-02, -1.3602e-02,\n",
      "        -2.5961e-02,  4.4702e-03,  3.3555e-04, -6.3212e-04,  5.0935e-03,\n",
      "         1.4303e-02, -2.5580e-02, -1.7354e-02, -7.9965e-03,  1.5514e-02,\n",
      "         1.1914e-02, -6.0114e-03,  2.9462e-02, -7.4307e-03,  1.6553e-02,\n",
      "        -2.8933e-02, -2.5666e-03, -3.0904e-02,  2.6598e-02, -2.5445e-02,\n",
      "        -2.8115e-02, -9.3111e-03,  2.6844e-02, -2.5072e-02,  2.4178e-02,\n",
      "         2.1124e-02, -2.2219e-02, -9.8574e-03,  2.1545e-02, -8.4712e-03,\n",
      "         1.5993e-02, -1.5387e-02, -2.1783e-02, -9.0777e-03,  3.5162e-02,\n",
      "        -1.3456e-03,  1.6936e-02,  3.6355e-03,  4.4705e-03,  6.7952e-03,\n",
      "        -1.1676e-02,  7.0887e-03, -3.2209e-02, -3.1086e-02, -1.1544e-02,\n",
      "        -2.6105e-02,  3.4707e-02, -1.1106e-02, -2.9744e-02, -2.3252e-02,\n",
      "         2.2424e-02, -2.6348e-02, -1.4646e-03,  1.1108e-02, -2.6792e-02,\n",
      "         1.0112e-02,  1.9487e-02,  5.0243e-03,  2.8154e-02,  2.3682e-02,\n",
      "        -1.5385e-02, -7.3115e-03, -2.7795e-02, -2.6390e-02,  1.8771e-02,\n",
      "        -1.6696e-02, -1.3118e-02,  3.3428e-02, -2.2173e-02, -2.3471e-02,\n",
      "        -2.0065e-02,  1.8221e-03, -1.1520e-02,  3.3261e-02,  1.8491e-02,\n",
      "         2.6872e-02, -2.1934e-02,  3.3917e-02, -5.0136e-03, -7.5772e-03,\n",
      "        -2.8310e-02,  2.0537e-02, -1.0831e-02, -1.8424e-02,  6.8881e-03,\n",
      "        -3.1315e-02, -1.5657e-02,  7.2473e-03, -3.1417e-02,  6.6842e-03,\n",
      "        -7.4698e-03, -1.9598e-02,  1.3901e-02, -8.6341e-03, -3.3342e-02,\n",
      "         1.3430e-02,  2.1909e-02, -2.3650e-02, -3.4056e-02, -1.9204e-02,\n",
      "        -1.3236e-03, -2.1354e-02,  9.5616e-03, -4.8832e-03, -2.0070e-02,\n",
      "         1.0987e-02,  9.2565e-03,  2.7127e-02,  2.8514e-02, -3.1873e-02,\n",
      "         5.7012e-03, -9.1385e-03]), 'bn1.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'bn1.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]), 'bn1.running_mean': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]), 'bn1.running_var': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.]), 'bn1.num_batches_tracked': tensor(0), 'layer2.weight': tensor([[ 0.0370,  0.0355, -0.0370,  ..., -0.0096, -0.0335,  0.0060],\n",
      "        [-0.0272,  0.0295, -0.0382,  ..., -0.0122,  0.0145,  0.0315],\n",
      "        [-0.0353, -0.0058,  0.0287,  ..., -0.0365,  0.0381, -0.0002],\n",
      "        ...,\n",
      "        [-0.0225,  0.0337, -0.0437,  ..., -0.0409,  0.0178,  0.0249],\n",
      "        [-0.0395,  0.0339, -0.0395,  ..., -0.0226, -0.0132,  0.0072],\n",
      "        [ 0.0135,  0.0219,  0.0084,  ..., -0.0087,  0.0026, -0.0044]]), 'layer2.bias': tensor([ 1.0843e-02,  2.8863e-02, -1.8079e-02, -4.2943e-02, -1.2821e-02,\n",
      "         1.0615e-02,  2.9197e-03,  2.2530e-02,  2.4140e-02,  3.9754e-02,\n",
      "         3.7626e-02,  4.2209e-02,  3.9460e-02,  6.1529e-03,  1.5612e-02,\n",
      "        -4.2989e-02, -2.3818e-02,  3.0349e-03, -3.8943e-02, -8.2818e-03,\n",
      "         3.2812e-03, -3.8823e-02,  2.2519e-02, -2.0658e-03,  2.0364e-02,\n",
      "        -3.5281e-03, -3.2035e-02, -1.4350e-02,  4.2442e-02,  2.5395e-02,\n",
      "        -3.5785e-02,  2.4012e-02, -3.9249e-02,  3.4420e-02,  3.0201e-02,\n",
      "         1.6026e-02, -3.8292e-03,  1.3673e-02,  1.0917e-02,  2.8364e-03,\n",
      "        -1.0244e-02, -1.9996e-02,  1.3520e-02,  2.3558e-02,  3.9274e-02,\n",
      "        -1.8742e-02, -4.4005e-03,  9.4125e-04, -2.8011e-03,  2.7204e-02,\n",
      "         3.5566e-02,  1.0678e-02,  3.7798e-02,  1.7829e-03,  6.0827e-04,\n",
      "         4.3911e-02,  2.2291e-02, -2.8757e-02, -1.0469e-03, -4.3708e-02,\n",
      "         2.1365e-02, -2.2557e-02,  3.9136e-02, -2.8242e-02,  7.0367e-03,\n",
      "         1.4302e-02,  1.7915e-02, -2.7093e-02,  1.0641e-02, -3.4824e-02,\n",
      "        -1.6020e-02, -4.1521e-02, -1.5099e-02, -1.1760e-02,  1.7194e-03,\n",
      "         3.8999e-02,  2.1980e-02, -2.8442e-02,  3.2685e-02, -1.3094e-03,\n",
      "         2.2740e-02,  3.4924e-02,  8.5231e-03,  4.3171e-02,  1.1728e-02,\n",
      "         3.8366e-02,  3.9581e-02, -4.1033e-02,  3.7230e-02,  2.7958e-03,\n",
      "        -5.5941e-04,  2.9274e-02, -4.6239e-03, -4.2765e-02, -1.4859e-02,\n",
      "         2.1472e-02, -1.4775e-02,  3.5978e-02, -3.8552e-02,  1.6887e-02,\n",
      "        -3.2532e-02, -2.5695e-02,  3.3959e-02, -3.7350e-02, -4.2174e-02,\n",
      "        -2.1588e-05,  1.4054e-02, -2.4131e-02, -4.7784e-03, -2.7466e-02,\n",
      "        -9.0614e-03,  1.3610e-04,  8.8210e-03,  3.8924e-02,  3.7749e-03,\n",
      "        -1.2494e-02,  9.7057e-03,  3.9909e-02,  4.1421e-02, -3.1472e-02,\n",
      "         3.0080e-02,  3.6706e-02,  2.8221e-02, -2.1555e-02,  3.5889e-02,\n",
      "        -1.9670e-02,  3.2626e-02, -3.6500e-02, -4.0259e-02, -2.6905e-02,\n",
      "        -2.3265e-02, -4.0435e-02, -3.5565e-02, -1.0420e-02,  4.2401e-02,\n",
      "         2.6521e-02,  1.7259e-02, -2.4186e-02,  1.0077e-02,  3.3201e-02,\n",
      "        -1.7004e-02, -4.1800e-02, -4.2880e-03, -3.4250e-02, -7.1775e-03,\n",
      "        -2.3532e-02,  4.3133e-02, -2.6406e-02, -8.2143e-03,  2.1908e-02,\n",
      "        -3.2764e-02, -3.8598e-03,  3.6679e-02,  7.2112e-03,  4.0884e-02,\n",
      "         3.3927e-02, -2.3148e-02, -3.8819e-02,  4.4172e-02, -3.9916e-02,\n",
      "        -4.1199e-02,  6.0990e-03, -1.2009e-02, -3.2573e-03,  3.5641e-02,\n",
      "        -2.7547e-02, -2.8662e-02,  6.9975e-03, -1.2633e-02,  1.8696e-02,\n",
      "        -9.0108e-03, -1.2913e-02,  1.9840e-02, -1.7490e-03, -1.9897e-02,\n",
      "         3.4265e-02,  2.5371e-02,  2.2941e-02, -1.6385e-02,  2.2476e-02,\n",
      "         3.0045e-02, -2.4297e-02, -2.5170e-02, -4.2081e-02,  2.8684e-02,\n",
      "         1.3956e-03, -4.1225e-02, -2.4752e-02,  1.5847e-02, -5.0984e-04,\n",
      "         1.4376e-02, -1.4420e-02, -2.5164e-03,  5.9174e-03,  1.3538e-02,\n",
      "        -1.8712e-02,  2.8744e-02,  1.9073e-02,  4.9163e-03,  3.8901e-02,\n",
      "        -2.1579e-02, -2.2525e-02,  1.1318e-02, -3.0018e-02,  5.1112e-03,\n",
      "         2.9354e-02, -3.3030e-02, -2.1805e-02,  1.2170e-02,  1.4299e-02,\n",
      "        -4.2083e-02, -1.2090e-02, -3.1024e-02,  2.7431e-02,  3.8103e-02,\n",
      "         3.1124e-02,  1.1386e-02, -2.6532e-02, -2.6063e-02, -1.1133e-03,\n",
      "         7.9472e-03, -4.2231e-02,  3.7215e-02, -2.1351e-02,  3.6666e-02,\n",
      "         2.9610e-02, -1.3314e-02,  3.9362e-02,  5.4710e-03,  9.8288e-03,\n",
      "         2.7158e-02, -2.5496e-02, -3.7931e-02, -1.5346e-02, -4.0042e-02,\n",
      "        -1.5713e-02,  1.3640e-02,  2.2946e-02,  2.6316e-02, -1.9666e-02,\n",
      "         3.1790e-02, -4.3138e-02,  6.3603e-03,  3.2994e-02, -2.8286e-02,\n",
      "         3.3766e-02, -6.9590e-03,  2.6034e-03, -1.4491e-02, -5.4505e-03,\n",
      "        -3.4980e-02, -2.0710e-02, -1.1988e-02,  2.8276e-02,  4.4178e-02,\n",
      "         2.0709e-02]), 'bn2.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.]), 'bn2.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'bn2.running_mean': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'bn2.running_var': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.]), 'bn2.num_batches_tracked': tensor(0), 'layer3.weight': tensor([[-0.0021,  0.0568,  0.0314,  ...,  0.0325, -0.0224, -0.0573],\n",
      "        [-0.0606,  0.0367, -0.0300,  ..., -0.0621, -0.0348,  0.0470],\n",
      "        [ 0.0331,  0.0021,  0.0056,  ..., -0.0454,  0.0409,  0.0596],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0374, -0.0160,  ...,  0.0161,  0.0432, -0.0001],\n",
      "        [ 0.0253,  0.0275,  0.0110,  ..., -0.0167, -0.0114, -0.0446],\n",
      "        [-0.0460, -0.0105, -0.0242,  ..., -0.0122,  0.0026,  0.0351]]), 'layer3.bias': tensor([-0.0448, -0.0408, -0.0418,  0.0309, -0.0201,  0.0370, -0.0053,  0.0064,\n",
      "         0.0090, -0.0118])})\n"
     ]
    }
   ],
   "source": [
    "class mnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.layer3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.layer1(x))\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(self.layer2(x))\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "model = mnistModel().to(device)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3360/3360 [00:34<00:00, 98.50batch/s, accuracy=0.88, loss=0.386]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.3856, Accuracy: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3360/3360 [00:32<00:00, 102.97batch/s, accuracy=0.926, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.2411, Accuracy: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3360/3360 [00:32<00:00, 103.96batch/s, accuracy=0.938, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.1995, Accuracy: 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3360/3360 [00:35<00:00, 95.56batch/s, accuracy=0.946, loss=0.178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.1776, Accuracy: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3360/3360 [00:37<00:00, 89.01batch/s, accuracy=0.952, loss=0.156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.1550, Accuracy: 0.9521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3360/3360 [00:28<00:00, 117.95batch/s, accuracy=0.955, loss=0.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.1428, Accuracy: 0.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3360/3360 [00:30<00:00, 109.50batch/s, accuracy=0.958, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.1342, Accuracy: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3360/3360 [00:32<00:00, 102.32batch/s, accuracy=0.962, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.1216, Accuracy: 0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3360/3360 [00:34<00:00, 96.57batch/s, accuracy=0.964, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.1147, Accuracy: 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3360/3360 [00:32<00:00, 102.36batch/s, accuracy=0.966, loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.1070, Accuracy: 0.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "    correct = 0\n",
    "    total_preds = 0\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch') as t:\n",
    "        for X, y in t :\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            logit = model(X)\n",
    "            loss = loss_func(logit, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(logit, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total_preds += y.size(0)\n",
    "            \n",
    "            # Cập nhật description của tqdm\n",
    "            t.set_postfix(loss=running_loss / (t.n + 1), accuracy=correct / total_preds)\n",
    "\n",
    "    # In kết quả sau mỗi epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total_preds\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Validation): 100%|██████████| 840/840 [00:02<00:00, 302.36batch/s, val_accuracy=0.979, val_loss=0.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Validation Loss: 0.0679, Validation Accuracy: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Chuyển mô hình sang chế độ evaluation (không cập nhật trọng số)\n",
    "val_loss = 0.0\n",
    "correct_val = 0\n",
    "total_val_preds = 0\n",
    "\n",
    "with torch.no_grad():  # Tắt tính toán gradient\n",
    "    with tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs} (Validation)', unit='batch') as t:\n",
    "        for X_val, y_val in t:\n",
    "            X_val = X_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            # Tính toán output từ mô hình\n",
    "            logit_val = model(X_val)\n",
    "\n",
    "            # Tính toán loss trên tập validation\n",
    "            loss_val = loss_func(logit_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            # Dự đoán lớp với giá trị logit cao nhất\n",
    "            _, predicted_val = torch.max(logit_val, 1)\n",
    "            correct_val += (predicted_val == y_val).sum().item()\n",
    "            total_val_preds += y_val.size(0)\n",
    "\n",
    "            # Cập nhật thông tin trong tqdm\n",
    "            t.set_postfix(val_loss=val_loss / (t.n + 1), val_accuracy=correct_val / total_val_preds)\n",
    "\n",
    "# Tính toán kết quả trên tập validation sau mỗi epoch\n",
    "val_epoch_loss = val_loss / len(test_loader)\n",
    "val_epoch_accuracy = correct_val / total_val_preds\n",
    "print(f\"Epoch {epoch+1}/{epochs} - Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
